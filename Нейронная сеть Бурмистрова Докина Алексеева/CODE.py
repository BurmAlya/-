import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import random


print("=" * 100)
print("üöÄ –ì–ò–ë–†–ò–î–ù–ê–Ø CNN-LSTM –ú–û–î–ï–õ–¨: –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤")
print("=" * 100)


# ============================================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================
config = {
    "max_words": 10000,
    "max_len": 300,              # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    "train_size": 0.8,
    "val_size": 0.1,
    "embedding_dim": 128,
    "cnn_filters": 64,           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CNN —Ñ–∏–ª—å—Ç—Ä–æ–≤
    "kernel_size": 5,            # –†–∞–∑–º–µ—Ä kernel –¥–ª—è CNN
    "lstm_units": 128,
    "dense_units": 128,
    "dropout_rate": 0.3,
    "spatial_dropout": 0.2,
    "batch_size": 32,
    "epochs": 50,
    "initial_learning_rate": 1e-3,
    "optimizer": "adam",
    "loss": "binary_crossentropy",
    "early_stopping_patience": 6,
    "reduce_lr_patience": 3,
    "reduce_lr_factor": 0.5,
    "random_seed": 42
}


tf.random.set_seed(config["random_seed"])
np.random.seed(config["random_seed"])
random.seed(config["random_seed"])


print("‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏:")
for k, v in config.items():
    print(f"  {k}: {v}")


# ============================================================
# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
# ============================================================
print("\nüìö –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê IMDB...")

from tensorflow.keras.datasets import imdb

(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = imdb.load_data(num_words=config["max_words"])
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(x_train_raw) + len(x_test_raw)} –æ—Ç–∑—ã–≤–æ–≤")


# ============================================================
# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ============================================================
from tensorflow.keras.preprocessing.sequence import pad_sequences

all_data = np.concatenate([x_train_raw, x_test_raw], axis=0)
all_labels = np.concatenate([y_train_raw, y_test_raw], axis=0)

all_data = pad_sequences(all_data, maxlen=config["max_len"], padding="post", truncating="post")

num_samples = len(all_data)
train_end = int(num_samples * config["train_size"])
val_end = int(num_samples * (config["train_size"] + config["val_size"]))

x_train = all_data[:train_end]
y_train = all_labels[:train_end]

x_val = all_data[train_end:val_end]
y_val = all_labels[train_end:val_end]

x_test = all_data[val_end:]
y_test = all_labels[val_end:]

print("\n‚úÖ –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫:")
print(f"  –û–±—É—á–∞—é—â–∞—è: {len(x_train)}")
print(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è: {len(x_val)}")
print(f"  –¢–µ—Å—Ç–æ–≤–∞—è: {len(x_test)}")


# tf.data –ø–∞–π–ø–ª–∞–π–Ω—ã
def make_dataset(features, labels, batch_size, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices((features, labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(features), reshuffle_each_iteration=True)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds


train_ds = make_dataset(x_train, y_train, config["batch_size"], shuffle=True)
val_ds = make_dataset(x_val, y_val, config["batch_size"], shuffle=False)
test_ds = make_dataset(x_test, y_test, config["batch_size"], shuffle=False)


# ============================================================
# 4. –ì–ò–ë–†–ò–î–ù–ê–Ø CNN-LSTM –ú–û–î–ï–õ–¨
# ============================================================
def build_cnn_lstm_model(config):
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
    - CNN –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    - LSTM –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    """
    inputs = keras.Input(shape=(config["max_len"],), name="text_input")

    # Embedding layer
    x = layers.Embedding(
        input_dim=config["max_words"],
        output_dim=config["embedding_dim"],
        input_length=config["max_len"],
        mask_zero=False
    )(inputs)
    x = layers.SpatialDropout1D(config["spatial_dropout"])(x) 
# CNN —Å–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    x = layers.Conv1D(
        filters=config["cnn_filters"],
        kernel_size=config["kernel_size"],
        activation='relu',
        padding='same'
    )(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(pool_size=2)(x)
    x = layers.Dropout(config["dropout_rate"])(x)

    # –í—Ç–æ—Ä–æ–π CNN —Å–ª–æ–π
    x = layers.Conv1D(
        filters=config["cnn_filters"] * 2,
        kernel_size=config["kernel_size"],
        activation='relu',
        padding='same'
    )(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(pool_size=2)(x)
    x = layers.Dropout(config["dropout_rate"])(x)

    # Bidirectional LSTM –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    x = layers.Bidirectional(
        layers.LSTM(config["lstm_units"], return_sequences=False)
    )(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(config["dropout_rate"])(x)

    # Dense —Å–ª–æ–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    x = layers.Dense(config["dense_units"], activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(config["dropout_rate"])(x)

    outputs = layers.Dense(1, activation="sigmoid", name="sentiment_output")(x)

    model = keras.Model(inputs=inputs, outputs=outputs, name="cnn_lstm_sentiment")

    # –í–ê–ñ–ù–û: —á–∏—Å–ª–æ–≤–æ–π learning_rate, –±–µ–∑ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
    opt = keras.optimizers.Adam(learning_rate=config["initial_learning_rate"])

    model.compile(
        optimizer=opt,
        loss=config["loss"],
        metrics=[
            "accuracy",
            keras.metrics.Precision(name="precision"),
            keras.metrics.Recall(name="recall"),
            keras.metrics.AUC(name="auc")
        ]
    )

    return model


print("\nüèó –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–ò–ë–†–ò–î–ù–û–ô CNN-LSTM –ú–û–î–ï–õ–ò...")
model = build_cnn_lstm_model(config)
model.summary()


# ============================================================
# 5. –ö–æ–ª–±—ç–∫–∏
# ============================================================
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=config["early_stopping_patience"],
    restore_best_weights=True,
    verbose=1
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=config["reduce_lr_factor"],
    patience=config["reduce_lr_patience"],
    min_lr=1e-7,
    verbose=1
)


# ============================================================
# 6. –û–±—É—á–µ–Ω–∏–µ
# ============================================================
print("\nüéì –û–ë–£–ß–ï–ù–ò–ï –ì–ò–ë–†–ò–î–ù–û–ô –ú–û–î–ï–õ–ò...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=config["epochs"],
    callbacks=[early_stopping, reduce_lr],
    verbose=1,
    steps_per_epoch=100
)

print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n")


# ============================================================
# 7. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
# ============================================================
print("üìä –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï...")
test_metrics = model.evaluate(test_ds, verbose=1)
print("\n" + "=" * 60)
print("–§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:")
print("=" * 60)
for name, value in zip(model.metrics_names, test_metrics):
    print(f"  {name.upper()}: {value:.4f}")
print("=" * 60)


# ============================================================
# 8. –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô
# ============================================================
print("\nüîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\n")

# –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
word_index = imdb.get_word_index()
reverse_word_index = {value: key for key, value in word_index.items()}

def decode_review(encoded_review):
    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i >= 3])

# –í—ã–±–∏—Ä–∞–µ–º —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
num_examples = 15
sample_indices = np.random.choice(len(x_test), num_examples, replace=False)

predictions = model.predict(x_test[sample_indices], verbose=0)
# –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
results = []
for idx, i in enumerate(sample_indices):
    review_text = decode_review(x_test[i])
    real_label = "–ü–æ–∑–∏—Ç–∏–≤" if y_test[i] == 1 else "–ù–µ–≥–∞—Ç–∏–≤"
    pred_prob = predictions[idx][0]
    pred_label = "–ü–æ–∑–∏—Ç–∏–≤" if pred_prob > 0.5 else "–ù–µ–≥–∞—Ç–∏–≤"
    correct = "‚úì" if real_label == pred_label else "‚úó"

    # –ü–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–∑—ã–≤–∞
    review_short = (review_text[:147] + '...') if len(review_text) > 150 else review_text

    results.append({
        '‚Ññ': idx + 1,
        '–û—Ç–∑—ã–≤ (–ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤)': review_short,
        '–†–µ–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å': real_label,
        '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å': pred_label,
        '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å': f'{pred_prob:.4f}',
        '–í–µ—Ä–Ω–æ': correct
    })

# –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É
df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
correct_count = sum(1 for r in results if r['–í–µ—Ä–Ω–æ'] == '‚úì')
print(f"\n\nüìà –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –∏–∑ {num_examples} –ø—Ä–∏–º–µ—Ä–æ–≤: {correct_count}/{num_examples} ({100*correct_count/num_examples:.1f}%)")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
df_results.to_csv('prediction_examples.csv', index=False, encoding='utf-8')
print("\n‚úÖ –ü—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: prediction_examples.csv")


 #============================================================
# 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
# ============================================================
train_loss = history.history["loss"]
val_loss = history.history["val_loss"]
train_acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]

# Loss
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_loss, label="train_loss")
plt.plot(val_loss, label="val_loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(train_acc, label="train_acc")
plt.plot(val_acc, label="val_acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('cnn_lstm_training_history.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: cnn_lstm_training_history.png")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
model.save('cnn_lstm_sentiment_model.keras')
print("\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: cnn_lstm_sentiment_model.keras")
