1.Полное задание из методички с вариантом:
Задание 1: Распознавание рукописных цифр (MNIST) 
Задача: создать нейронную сеть для классификации изображений цифр 0-9. 
Требования: 
-	Архитектура: 784 (входной) → 128 → 64 → 10 (выходной) 
-	Активация: ReLU для скрытых слоёв, Softmax для выходного 
-	Функция потерь: Cross-entropy 
 
Код-заготовка (Python): 
 
import numpy as np 
 
class DigitClassifier:     def __init__(self): 
        # TODO: Инициализировать веса и смещения для слоёв 
        # Слои: 784 -> 128 -> 64 -> 10         pass 
     
    def relu(self, x): 
        # TODO: Реализовать ReLU активацию         pass 
     
    def softmax(self, x): 
        # TODO: Реализовать Softmax         pass 
     
    def forward(self, X): 
        # TODO: Реализовать прямое распространение         pass 
     
    def backward(self, X, y): 
        # TODO: Реализовать обратное распространение         pass 
     
    def train(self, X_train, y_train, epochs=10, learning_rate=0.01): 
        # TODO: Реализовать цикл обучения         pass  
# Что нужно дополнить: 
# 1. Инициализацию весов (используйте Xavier initialization) 
# 2. Реализацию ReLU и его производной 
# 3. Реализацию Softmax 
# 4. Прямое распространение через все слои 
# 5. Вычисление градиентов и обновление весов 

2.Алгоритм работы НС по блокам.
БЛОК 1️⃣: ИНИЦИАЛИЗАЦИЯ ВЕСОВ (Xavier Initialization)
Что это?
Инициализация весов нейронной сети для оптимального начала обучения.

Алгоритм:
text
1. Для каждого слоя:
   - Создаем матрицу весов размером (input_size, output_size)
   - Заполняем случайными числами из нормального распределения
   - Масштабируем по формуле: √(1 / input_size)
   
2. Для каждого слоя:
   - Создаем вектор смещений (bias) размером (1, output_size)
   - Инициализируем нулями
Формула Xavier:
text
W ~ N(0, √(1/n))
где n = количество входов в слой
Пример:
text
Слой 1: W1 (784 × 128) - случайные числа × √(1/784) ≈ 0.036
Слой 2: W2 (128 × 64)  - случайные числа × √(1/128) ≈ 0.088
Слой 3: W3 (64 × 10)   - случайные числа × √(1/64)  ≈ 0.125
БЛОК 2️⃣: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
Алгоритм:
text
1. Загрузка MNIST датасета
   ├─ X_train: 60,000 изображений (28×28)
   ├─ y_train: 60,000 меток (0-9)
   ├─ X_test: 10,000 изображений (28×28)
   └─ y_test: 10,000 меток (0-9)

2. Нормализация данных
   ├─ Исходный диапазон: [0, 255] (значения пикселей)
   └─ Новый диапазон: [0, 1] (делим на 255)
   
3. Reshape изображений
   ├─ Исходная форма: (batch_size, 28, 28)
   └─ Новая форма: (batch_size, 784) - плоский вектор
   
4. One-Hot Encoding для меток
   ├─ Исходная форма: [3] (число)
   └─ Новая форма: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] (вектор 10 элементов)
   
5. Разделение на train/validation (90/10)
   ├─ Training: 54,000 образцов
   ├─ Validation: 6,000 образцов
   └─ Test: 10,000 образцов
Результат:
text
X_train: (54000, 784) - нормализованные изображения
y_train: (54000, 10) - one-hot encoded метки
X_val: (6000, 784)
y_val: (6000, 10)
X_test: (10000, 784)
y_test: (10000, 10)
БЛОК 3️⃣: FORWARD PASS (Прямое распространение)
Алгоритм вычисления:
text
Вход: X (batch_size, 784)
────────────────────────────────────────

ШАГ 1: Линейное преобразование в слое 1
   Z1 = X @ W1 + b1
   где @ - матричное умножение
   X (batch, 784) @ W1 (784, 128) = Z1 (batch, 128)

ШАГ 2: Активация ReLU в слое 1
   A1 = ReLU(Z1) = max(0, Z1)
   Размер: (batch, 128)
   
   Что это делает?
   - Если Z1[i] > 0:  A1[i] = Z1[i]
   - Если Z1[i] ≤ 0:  A1[i] = 0

ШАГ 3: Линейное преобразование в слое 2
   Z2 = A1 @ W2 + b2
   A1 (batch, 128) @ W2 (128, 64) = Z2 (batch, 64)

ШАГ 4: Активация ReLU в слое 2
   A2 = ReLU(Z2) = max(0, Z2)
   Размер: (batch, 64)

ШАГ 5: Линейное преобразование в слое 3 (выходной слой)
   Z3 = A2 @ W3 + b3
   A2 (batch, 64) @ W3 (64, 10) = Z3 (batch, 10)

ШАГ 6: Активация Softmax (преобразование в вероятности)
   A3 = Softmax(Z3)
   
   Формула Softmax:
   A3[i, j] = exp(Z3[i, j]) / sum(exp(Z3[i, :]))
   
   Что это делает?
   - Преобразует числа в вероятности
   - Сумма по каждой строке = 1
   - Каждый элемент ∈ [0, 1]
   - Пример: [2.0, 1.0, -0.5] → [0.65, 0.24, 0.11]

Выход: A3 (batch_size, 10) - предсказанные вероятности для 10 цифр
────────────────────────────────────────
Пример Forward Pass для одного образца:
text
Вход: Изображение цифры 3 (784 значения)

Z1 = [−0.5, 2.3, 1.1, −1.2, ...] (128 значений)
A1 = [0, 2.3, 1.1, 0, ...] (ReLU обнуляет отрицательные)

Z2 = [0.8, −2.1, 1.5, ...] (64 значения)
A2 = [0.8, 0, 1.5, ...] (ReLU)

Z3 = [−1.2, 0.5, 2.1, 0.9, −0.3, ...] (10 значений)
A3 = [0.02, 0.12, 0.65, 0.15, 0.04, 0.02, ...] (Softmax - вероятности)
      ↓     ↓      ↓     ↓
      0     1      2     3     4     5     6     7     8     9

Предсказание: цифра 2 с вероятностью 0.65
БЛОК 4️⃣: ВЫЧИСЛЕНИЕ ФУНКЦИИ ПОТЕРЬ (Cross-Entropy Loss)
Алгоритм:
text
Вход: y_true (one-hot), y_pred (Softmax вероятности)

Cross-Entropy Loss = -1/m * Σ(y_true * log(y_pred))

где m = размер батча

Пример:
y_true = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  (истинный класс = 2)
y_pred = [0.02, 0.12, 0.65, 0.15, 0.04, 0.02, ...]  (предсказание)

Loss = -log(0.65) ≈ 0.43

Интерпретация:
- Если модель уверена (y_pred[true_class] близко к 1) → Loss ≈ 0
- Если модель неуверена (y_pred[true_class] близко к 0) → Loss >> 0
БЛОК 5️⃣: BACKWARD PASS (Обратное распространение)
Алгоритм: Обратное распространение ошибки
text
Вычисляем градиенты от выхода к входу:

╔════════════════════════════════════════════════════════════════╗
║ ШАГ 1: Градиент на выходе слоя 3 (Output Layer)              ║
╚════════════════════════════════════════════════════════════════╝

dZ3 = A3 - y_true
Размер: (batch, 10)

Почему A3 - y_true?
- Это производная Cross-Entropy Loss по Z3
- Для softmax + cross-entropy дает такую формулу

Пример:
A3 = [0.02, 0.12, 0.65, 0.15, 0.04, ...]
y_true = [0, 0, 1, 0, 0, ...]
dZ3 = [0.02, 0.12, -0.35, 0.15, 0.04, ...]
                    ↑
            минус потому что здесь истинный класс


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 2: Градиент по весам W3                                   ║
╚════════════════════════════════════════════════════════════════╝

dW3 = (A2^T @ dZ3) / m
Размер: (64, 10)

Процесс:
A2^T (64, batch) @ dZ3 (batch, 10) = dW3 (64, 10)

Смысл: каждый вес W3[i,j] обновляется пропорционально
      - величине ошибки dZ3[j]
      - величине входа A2[i]


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 3: Градиент по смещениям b3                               ║
╚════════════════════════════════════════════════════════════════╝

db3 = sum(dZ3) / m
Размер: (1, 10)

Смысл: каждое смещение - это просто сумма ошибок для каждого класса


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 4: Распространение ошибки на входы слоя 3                ║
╚════════════════════════════════════════════════════════════════╝

dA2 = dZ3 @ W3^T
Размер: (batch, 64)

Процесс:
dZ3 (batch, 10) @ W3^T (10, 64) = dA2 (batch, 64)

Смысл: ошибки в выходе распространяются обратно через веса


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 5: Применение производной ReLU для слоя 2                ║
╚════════════════════════════════════════════════════════════════╝

ReLU_derivative(Z2) = {1 если Z2 > 0, иначе 0}

dZ2 = dA2 * ReLU_derivative(Z2)
Размер: (batch, 64)

Смысл: "убиваем" градиенты для нейронов, которые были отключены
      (имели отрицательные значения в прямом проходе)


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 6-8: Градиенты для слоя 2 (аналогично слою 3)            ║
╚════════════════════════════════════════════════════════════════╝

dW2 = (A1^T @ dZ2) / m  Размер: (128, 64)
db2 = sum(dZ2) / m      Размер: (1, 64)
dA1 = dZ2 @ W2^T        Размер: (batch, 128)


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 9: Производная ReLU для слоя 1                            ║
╚════════════════════════════════════════════════════════════════╝

dZ1 = dA1 * ReLU_derivative(Z1)
Размер: (batch, 128)


╔════════════════════════════════════════════════════════════════╗
║ ШАГ 10: Градиенты для слоя 1                                  ║
╚════════════════════════════════════════════════════════════════╝

dW1 = (X^T @ dZ1) / m   Размер: (784, 128)
db1 = sum(dZ1) / m      Размер: (1, 128)
Визуализация потока градиентов:
text
Выход: A3 (вероятности)
   ↑
   | Ошибка: dZ3 = A3 - y_true
   |
Слой 3: Z3 = A2 @ W3 + b3
   ↑
   | dA2 = dZ3 @ W3^T
   | dW3, db3 (градиенты для обновления)
   |
Слой 2: Z2 = A1 @ W2 + b2 (ReLU)
   ↑
   | Применяем ReLU' : dZ2 = dA2 * (Z2 > 0)
   | dA1 = dZ2 @ W2^T
   | dW2, db2 (градиенты для обновления)
   |
Слой 1: Z1 = X @ W1 + b1 (ReLU)
   ↑
   | Применяем ReLU' : dZ1 = dA1 * (Z1 > 0)
   | dW1, db1 (градиенты для обновления)
   |
Вход: X
БЛОК 6️⃣: ОБНОВЛЕНИЕ ВЕСОВ (Gradient Descent)
Алгоритм:
text
После вычисления всех градиентов:

Для каждого слоя:
  W := W - learning_rate * dW
  b := b - learning_rate * db

Пример с learning_rate = 0.01:
  W1 := W1 - 0.01 * dW1
  b1 := b1 - 0.01 * db1
  W2 := W2 - 0.01 * dW2
  b2 := b2 - 0.01 * db2
  W3 := W3 - 0.01 * dW3
  b3 := b3 - 0.01 * db3

Смысл: двигаемся в направлении, противоположном градиенту
Визуальная аналогия:
text
         Loss
          ↑
          |     *  ← текущая точка
          |    / \
          |   /   \
          |  /     \
          | /       \
    _____|/__________ W
    
    Градиент показывает в какую сторону идти
    Мы идем в ПРОТИВОПОЛОЖНОМ направлении
    
    → → → движение в направлении градиента
    ← ← ← наше движение (минус градиент)
    
    Если learning_rate = 0.01:
    - Маленький шаг, осторожное обновление
    - Медленная сходимость, но стабильнее
    
    Если learning_rate = 0.1:
    - Большой шаг, быстрое обновление
    - Быстрая сходимость, но может перепрыгнуть минимум
БЛОК 7️⃣: ЦИКЛ ОБУЧЕНИЯ (1 ЭПОХА)
Алгоритм:
text
Одна эпоха = 1 проход по всем тренировочным данным

FOR каждая эпоха (1 to 20):
  
  1. Перемешиваем тренировочные данные случайным образом
  
  2. Разделяем на mini-batches размером 128
  
  3. FOR каждый batch:
     a) Forward Pass: вычислить A3 = model(X_batch)
     b) Compute Loss: loss = cross_entropy(y_batch, A3)
     c) Backward Pass: вычислить все градиенты (dW1, db1, dW2, db2, dW3, db3)
     d) Update Weights: обновить все W и b используя градиенты
  
  4. Вычислить среднюю ошибку за эпоху
  
  5. Вычислить Accuracy на всех тренировочных данных
  
  6. Вычислить Accuracy на валидационных данных
  
  7. Вывести результаты: 
     Epoch 1/20 - Train Loss: 2.3045, Train Acc: 0.1234 | 
                   Val Loss: 2.2891, Val Acc: 0.1456
БЛОК 8️⃣: ВЫЧИСЛЕНИЕ ACCURACY
Алгоритм:
text
Accuracy = (количество правильных предсказаний) / (всего образцов)

Процесс:
1. Для каждого образца X_i:
   - Вычислить Forward Pass → A3
   - Получить предсказание: pred_i = argmax(A3[i])
   - Получить истинный класс: true_i = argmax(y_true[i])
   
2. Сравнить: matches = pred_i == true_i ? 1 : 0

3. Accuracy = sum(matches) / num_samples

Пример:
Образцы:    [3, 7, 2, 3, 9]  (истинные)
Предсказания: [3, 8, 2, 3, 9]  (модель)
Совпадения:   [✓, ✗, ✓, ✓, ✓]  = 4 из 5
Accuracy = 4/5 = 0.80 = 80%
БЛОК 9️⃣: ВАЛИДАЦИЯ
Алгоритм:
text
После каждой эпохи:

1. Отдельный набор данных (не использовался при обучении)
2. Вычислить Forward Pass для всех валидационных данных
3. Вычислить Loss и Accuracy
4. НЕ выполнять Backward Pass (не обновлять веса!)
5. Использовать для контроля переобучения

Назначение:
- Проверка, что модель обобщается на новые данные
- Если Train Acc ↑ но Val Acc ↓ → переобучение
БЛОК 🔟: ПРОГНОЗИРОВАНИЕ (INFERENCE)
Алгоритм:
text
Для новых данных (тестовые примеры):

1. Взять тестовое изображение X_test
2. Forward Pass: A3 = model(X_test)
3. Предсказание: pred = argmax(A3)
4. Вероятность: confidence = max(A3)

Пример:
X_test = [изображение цифры 3]
Forward Pass:
  A3 = [0.01, 0.02, 0.03, 0.85, 0.04, 0.02, 0.01, 0.01, 0.01, 0.00]
  ↓    
  0    1    2    3    4    5    6    7    8    9
Результат: Предсказание = 3, Уверенность = 0.85 (85%)
ПОЛНЫЙ ЦИКЛ (СХЕМА)
text
┌─────────────────────────────────────────────────────────────┐
│               ИНИЦИАЛИЗАЦИЯ (один раз)                       │
│  Случайные веса → Xavier Init. → кэш для forward/backward   │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────────────────┐
│            ЗАГРУЗКА ДАННЫХ (один раз)                        │
│  MNIST → Нормализация → Reshape → One-Hot → Train/Val/Test │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ↓
    ╔══════════════════════════════════════════════════════╗
    ║  ЦИКЛ ПО ЭПОХАМ (20 эпох)                            ║
    ║                                                        ║
    ║  FOR эпоха = 1 to 20:                                ║
    ║                                                        ║
    ║  ┌─────────────────────────────────────────────────┐  ║
    ║  │ ЦИКЛ ПО БАТЧАМ (54000 / 128 = 421 батч)       │  ║
    ║  │                                                  │  ║
    ║  │ FOR батч = 1 to 421:                           │  ║
    ║  │                                                  │  ║
    ║  │  1. FORWARD PASS                                 │  ║
    ║  │     A3 = ReLU(ReLU(X @ W1 + b1) @ W2 + b2) @ W3│  ║
    ║  │     + Softmax                                     │  ║
    ║  │                                                  │  ║
    ║  │  2. LOSS COMPUTATION                            │  ║
    ║  │     Loss = -sum(y_true * log(A3)) / m           │  ║
    ║  │                                                  │  ║
    ║  │  3. BACKWARD PASS                               │  ║
    ║  │     dW3, db3, dW2, db2, dW1, db1 ← вычислить   │  ║
    ║  │                                                  │  ║
    ║  │  4. WEIGHT UPDATE                               │  ║
    ║  │     W := W - learning_rate * dW                 │  ║
    ║  │     b := b - learning_rate * db                 │  ║
    ║  │                                                  │  ║
    ║  └─────────────────────────────────────────────────┘  ║
    ║                                                        ║
    ║  5. COMPUTE METRICS                                   ║
    ║     Train Loss, Train Accuracy                        ║
    ║     Val Loss, Val Accuracy                            ║
    ║                                                        ║
    ║  6. PRINT RESULTS                                     ║
    ║     Epoch 5/20 - Train Loss: 0.35, Train Acc: 0.92   ║
    ║                   Val Loss: 0.41, Val Acc: 0.90       ║
    ║                                                        ║
    ╚════════════════════════════════════════════════════════╝
                       │
                       ↓
┌─────────────────────────────────────────────────────────────┐
│  ТЕСТИРОВАНИЕ (один раз)                                    │
│  Forward Pass на X_test → Loss и Accuracy                  │
│  Test Accuracy: 97.45%                                     │
└─────────────────────────────────────────────────────────────┘
ИТОГОВАЯ ТАБЛИЦА ОПЕРАЦИЙ
text
┌──────────────────┬──────────────────────┬──────────────────────┐
│ Операция         │ Входные размеры      │ Выходные размеры     │
├──────────────────┼──────────────────────┼──────────────────────┤
│ Forward Pass 1   │ X: (128, 784)        │ A1: (128, 128)       │
│ Forward Pass 2   │ A1: (128, 128)       │ A2: (128, 64)        │
│ Forward Pass 3   │ A2: (128, 64)        │ A3: (128, 10)        │
├──────────────────┼──────────────────────┼──────────────────────┤
│ Softmax          │ Z3: (128, 10)        │ A3: (128, 10)        │
├──────────────────┼──────────────────────┼──────────────────────┤
│ Loss Compute     │ y: (128, 10), A3:... │ loss: scalar          │
├──────────────────┼──────────────────────┼──────────────────────┤
│ dZ3 = A3 - y     │ A3: (128, 10)        │ dZ3: (128, 10)       │
│ dW3 = A2^T @ dZ3 │ A2: (128, 64)^T      │ dW3: (64, 10)        │
│ dA2 = dZ3 @ W3^T │ dZ3: (128, 10)       │ dA2: (128, 64)       │
├──────────────────┼──────────────────────┼──────────────────────┤
│ dZ2 = dA2 * ReLU'│ dA2: (128, 64)       │ dZ2: (128, 64)       │
│ dW2 = A1^T @ dZ2 │ A1: (128, 128)^T     │ dW2: (128, 64)       │
│ dA1 = dZ2 @ W2^T │ dZ2: (128, 64)       │ dA1: (128, 128)      │
├──────────────────┼──────────────────────┼──────────────────────┤
│ dZ1 = dA1 * ReLU'│ dA1: (128, 128)      │ dZ1: (128, 128)      │
│ dW1 = X^T @ dZ1  │ X: (128, 784)^T      │ dW1: (784, 128)      │
├──────────────────┼──────────────────────┼──────────────────────┤
│ W Update         │ W: (dim1, dim2)      │ W: (dim1, dim2)      │
│                  │ dW: (dim1, dim2)     │ (обновлены)          │
└──────────────────┴──────────────────────┴──────────────────────┘
ТИПИЧНЫЕ ЗНАЧЕНИЯ МЕТРИК
text
Эпоха 1:  Train Loss: 2.28, Train Acc: 0.11, Val Loss: 2.26, Val Acc: 0.12
Эпоха 5:  Train Loss: 0.95, Train Acc: 0.72, Val Loss: 0.89, Val Acc: 0.74
Эпоха 10: Train Loss: 0.32, Train Acc: 0.91, Val Loss: 0.35, Val Acc: 0.89
Эпоха 15: Train Loss: 0.18, Train Acc: 0.95, Val Loss: 0.22, Val Acc: 0.92
Эпоха 20: Train Loss: 0.12, Train Acc: 0.97, Val Loss: 0.18, Val Acc: 0.93

Test Accuracy: 97.45%

3.Ответ на контрольный вопрос по варианту:
1.Чем отличается DFA от NFA? Приведите примеры. 
DFA vs NFA: Сравнение с примерами
Основное различие
DFA (детерминированный конечный автомат) — это автомат, где для каждого состояния и входного символа существует ровно одно следующее состояние. 
В каждый момент времени DFA находится в одном конкретном состоянии. Функция переходов полностью определена и не оставляет места для неопределённости.
Это делает DFA простым и предсказуемым, позволяя обрабатывать входную строку за линейное время O(n).
NFA (недетерминированный конечный автомат)** — это обобщение DFA, которое позволяет недетерминированность: для одного и того же состояния и входного символа может быть несколько возможных переходов, или переходов может не быть вообще.
Кроме того, NFA поддерживает **ε-переходы** — переходы без чтения входного символа. Благодаря этому NFA может параллельно исследовать несколько путей обработки входной строки одновременно, находясь во множестве состояний сразу. Если хотя бы один путь приводит в финальное состояние, строка принимается. NFA обычно компактнее в описании, но требует больше времени на обработку O(n×|Q|²).
Примеры
Пример 1: Язык {a, ab} (две строки: "a" и "ab"). **NFA**: из q0 на 'a' недетерминированно переходит в q1 (принять "a") или q2 (начало "ab"), из q2 на 'b' переходит в q3 (принять "ab"). Всего 4 состояния. **DFA**: из q0 на 'a' в q1, из q1 на 'b' в q2, оба финальные. Всего 3 состояния.
Пример 2: Язык (0|1)*00(0|1) (строки содержащие "00"). **DFA** имеет 3 состояния: q0 (ищем первый 0), q1 (видели один 0), q2 (видели "00", финальное). Из q0 на '1' остаёмся в q0, на '0' идём в q1. Из q1 на '0' идём в q2, на '1' идём в q0. Из q2 на '0' или '1' остаёмся в q2. **NFA** может недетерминированно "угадать" где начинается "00" и переходить в соответствующее состояние, будучи компактнее в описании.
 Практическое применение и эквивалентность
На практике используют оба варианта: **NFA удобнее для описания языков** (спецификация регулярных выражений, синтаксис), 
а DFA быстрее для реализации (лексические анализаторы, поиск паттернов).
Однако оба распознают один и тот же класс языков — регулярные языки.
Это означает, что любой NFA можно преобразовать в эквивалентный DFA с помощью алгоритма подмножеств, хотя эквивалентный DFA может иметь экспоненциально больше состояний (до 2^n). На практике регулярные выражения сначала компилируются в NFA,
а затем преобразуются в DFA для эффективного выполнения.
