import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.utils import to_categorical

# ============================================
# DIGIT CLASSIFIER - РАСПОЗНАВАНИЕ РУКОПИСНЫХ ЦИФР
# ============================================

class DigitClassifier:
    """Нейронная сеть для классификации цифр 0-9"""
    
    def __init__(self):
        """Инициализация весов и смещений для слоёв
        Архитектура: 784 -> 128 -> 64 -> 10
        """
        # Xavier инициализация для весов
        self.W1 = np.random.randn(784, 128) * np.sqrt(1.0 / 784)
        self.b1 = np.zeros((1, 128))
        
        self.W2 = np.random.randn(128, 64) * np.sqrt(1.0 / 128)
        self.b2 = np.zeros((1, 64))
        
        self.W3 = np.random.randn(64, 10) * np.sqrt(1.0 / 64)
        self.b3 = np.zeros((1, 10))
        
        # Для кэширования значений во время forward pass
        self.cache = {}
    
    def relu(self, x):
        """Реализовать ReLU активацию: max(0, x)"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """Производная ReLU: 1 если x > 0, иначе 0"""
        return (x > 0).astype(float)
    
    def softmax(self, x):
        """Реализовать Softmax: exp(x) / sum(exp(x))"""
        # Численная стабильность: вычитаем max
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def cross_entropy_loss(self, y_true, y_pred):
        """Вычисление Cross-Entropy Loss
        y_true: one-hot encoded labels (batch_size, num_classes)
        y_pred: softmax predictions (batch_size, num_classes)
        """
        m = y_true.shape[0]
        # Clip predictions для избежания log(0)
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        loss = -np.sum(y_true * np.log(y_pred)) / m
        return loss
    
    def forward(self, X):
        """Реализовать прямое распространение (Forward Pass)
        X shape: (batch_size, 784)
        """
        # Слой 1: 784 -> 128
        z1 = np.dot(X, self.W1) + self.b1
        a1 = self.relu(z1)
        
        # Слой 2: 128 -> 64
        z2 = np.dot(a1, self.W2) + self.b2
        a2 = self.relu(z2)
        
        # Слой 3: 64 -> 10 (output layer)
        z3 = np.dot(a2, self.W3) + self.b3
        a3 = self.softmax(z3)
        
        # Кэшируем значения для backward pass
        self.cache = {
            'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'a3': a3
        }
        
        return a3
    
    def backward(self, X, y, learning_rate=0.01):
        """Реализовать обратное распространение (Backward Pass)
        Вычисление градиентов и обновление весов
        
        X shape: (batch_size, 784)
        y shape: (batch_size, 10) - one-hot encoded
        """
        m = X.shape[0]
        
        # Извлекаем кэшированные значения
        a1 = self.cache['a1']
        a2 = self.cache['a2']
        a3 = self.cache['a3']
        z1 = self.cache['z1']
        z2 = self.cache['z2']
        
        # ===== СЛОЙ 3 (Output Layer) =====
        # dL/da3 = a3 - y (для cross-entropy + softmax)
        dz3 = a3 - y
        dW3 = np.dot(a2.T, dz3) / m
        db3 = np.sum(dz3, axis=0, keepdims=True) / m
        da2 = np.dot(dz3, self.W3.T)
        
        # ===== СЛОЙ 2 =====
        dz2 = da2 * self.relu_derivative(z2)
        dW2 = np.dot(a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        da1 = np.dot(dz2, self.W2.T)
        
        # ===== СЛОЙ 1 =====
        dz1 = da1 * self.relu_derivative(z1)
        dW1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        # ===== ОБНОВЛЕНИЕ ВЕСОВ (Gradient Descent) =====
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        
        self.W3 -= learning_rate * dW3
        self.b3 -= learning_rate * db3
    
    def train(self, X_train, y_train, epochs=10, learning_rate=0.01, batch_size=32, 
              X_val=None, y_val=None):
        """Реализовать цикл обучения
        
        X_train shape: (num_samples, 784)
        y_train shape: (num_samples, 10) - one-hot encoded
        """
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        
        num_batches = len(X_train) // batch_size
        
        print(f"{'='*60}")
        print(f"ОБУЧЕНИЕ МОДЕЛИ")
        print(f"{'='*60}\n")
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            
            # Перемешиваем данные
            indices = np.random.permutation(len(X_train))
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]
            
            # Mini-batch gradient descent
            for batch in range(num_batches):
                start_idx = batch * batch_size
                end_idx = start_idx + batch_size
                
                X_batch = X_shuffled[start_idx:end_idx]
                y_batch = y_shuffled[start_idx:end_idx]
                
                # Forward pass
                y_pred = self.forward(X_batch)
                
                # Вычисляем loss
                loss = self.cross_entropy_loss(y_batch, y_pred)
                epoch_loss += loss
                
                # Backward pass
                self.backward(X_batch, y_batch, learning_rate)
            
            # Усредняем loss за эпоху
            epoch_loss /= num_batches
            train_losses.append(epoch_loss)
            
            # Вычисляем accuracy на training data
            y_pred_train = self.forward(X_train)
            train_acc = np.mean(np.argmax(y_pred_train, axis=1) == np.argmax(y_train, axis=1))
            train_accuracies.append(train_acc)
            
            # Validation (если предоставлены)
            if X_val is not None and y_val is not None:
                y_pred_val = self.forward(X_val)
                val_loss = self.cross_entropy_loss(y_val, y_pred_val)
                val_losses.append(val_loss)
                
                val_acc = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_val, axis=1))
                val_accuracies.append(val_acc)
                
                print(f"Epoch {epoch+1}/{epochs} - " +
                      f"Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f} | " +
                      f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            else:
                print(f"Epoch {epoch+1}/{epochs} - " +
                      f"Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}")
        
        print(f"\n{'='*60}")
        print(f"ОБУЧЕНИЕ ЗАВЕРШЕНО!")
        print(f"{'='*60}\n")
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accuracies': train_accuracies,
            'val_accuracies': val_accuracies
        }
    
    def predict(self, X):
        """Предсказание класса для новых данных"""
        y_pred = self.forward(X)
        return np.argmax(y_pred, axis=1)
    
    def evaluate(self, X_test, y_test):
        """Оценка модели на тестовом наборе"""
        y_pred = self.forward(X_test)
        loss = self.cross_entropy_loss(y_test, y_pred)
        
        predictions = np.argmax(y_pred, axis=1)
        labels = np.argmax(y_test, axis=1)
        accuracy = np.mean(predictions == labels)
        
        return loss, accuracy


# ============================================
# ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
# ============================================

print("="*60)
print("ЗАГРУЗКА MNIST ДАТАСЕТА")
print("="*60)

# Загружаем MNIST датасет
(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()

print(f"\nИсходные размеры данных:")
print(f"  - X_train: {X_train_raw.shape} (60,000 изображений 28x28)")
print(f"  - X_test: {X_test_raw.shape} (10,000 изображений 28x28)")
print(f"  - y_train: {y_train_raw.shape}")
print(f"  - y_test: {y_test_raw.shape}")

# Нормализация: [0, 255] -> [0, 1]
X_train = X_train_raw.astype(np.float32) / 255.0
X_test = X_test_raw.astype(np.float32) / 255.0

# Reshape: (batch_size, 28, 28) -> (batch_size, 784)
X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)

# One-hot encoding для labels
y_train = to_categorical(y_train_raw, 10)
y_test = to_categorical(y_test_raw, 10)

# Разделяем training set на train/validation (90/10)
val_split = int(0.9 * len(X_train))
X_train_split = X_train[:val_split]
y_train_split = y_train[:val_split]
X_val = X_train[val_split:]
y_val = y_train[val_split:]

print(f"\nПосле предработки:")
print(f"  - X_train: {X_train_split.shape} (normalized to [0, 1])")
print(f"  - X_val: {X_val.shape}")
print(f"  - X_test: {X_test.shape}")
print(f"  - y_train: {y_train_split.shape} (one-hot encoded)")
print(f"  - y_val: {y_val.shape}")
print(f"  - y_test: {y_test.shape}")

# ============================================
# СОЗДАНИЕ И ОБУЧЕНИЕ МОДЕЛИ
# ============================================

classifier = DigitClassifier()

# Обучение модели
history = classifier.train(
    X_train_split, y_train_split,
    epochs=20,
    learning_rate=0.01,
    batch_size=128,
    X_val=X_val,
    y_val=y_val
)

# ============================================
# ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ
# ============================================

print(f"{'='*60}")
print(f"ВИЗУАЛИЗАЦИЯ ГРАФИКОВ")
print(f"{'='*60}\n")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# График Loss
axes[0].plot(history['train_losses'], label='Train Loss', marker='o', linewidth=2)
axes[0].plot(history['val_losses'], label='Validation Loss', marker='s', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# График Accuracy
axes[1].plot(history['train_accuracies'], label='Train Accuracy', marker='o', linewidth=2)
axes[1].plot(history['val_accuracies'], label='Validation Accuracy', marker='s', linewidth=2)
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('Accuracy', fontsize=12)
axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("✓ Графики успешно отображены!")

# ============================================
# ТЕСТИРОВАНИЕ НА HOLD-OUT ТЕСТЕ
# ============================================

print(f"\n{'='*60}")
print(f"ТЕСТИРОВАНИЕ НА HOLD-OUT ТЕСТЕ")
print(f"{'='*60}\n")

test_loss, test_accuracy = classifier.evaluate(X_test, y_test)

print(f"Результаты на тестовом наборе:")
print(f"  - Test Loss: {test_loss:.4f}")
print(f"  - Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# ============================================
# ДЕМОНСТРАЦИЯ ПРЕДСКАЗАНИЙ
# ============================================

print(f"\n{'='*60}")
print(f"ДЕМОНСТРАЦИЯ ПРЕДСКАЗАНИЙ")
print(f"{'='*60}\n")

# Выбираем случайные примеры
num_examples = 10
indices = np.random.choice(len(X_test), num_examples, replace=False)

fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.flatten()

for idx, sample_idx in enumerate(indices):
    # Получаем предсказание
    prediction = classifier.predict(X_test[sample_idx:sample_idx+1])[0]
    true_label = y_test_raw[sample_idx]
    
    # Визуализируем изображение
    image = X_test_raw[sample_idx]
    axes[idx].imshow(image, cmap='gray')
    
    # Окрашиваем в зеленый если правильно, красный если неправильно
    color = 'green' if prediction == true_label else 'red'
    axes[idx].set_title(f'Pred: {prediction}\nTrue: {true_label}', color=color, fontweight='bold')
    axes[idx].axis('off')

plt.tight_layout()
plt.show()

# ============================================
# ИТОГОВАЯ СТАТИСТИКА
# ============================================

print(f"\n{'='*60}")
print(f"✓ ВСЕ КОМПОНЕНТЫ УСПЕШНО РЕАЛИЗОВАНЫ!")
print(f"{'='*60}")
print(f"\nРеализованные компоненты:")
print(f"  ✓ Xavier инициализация весов")
print(f"  ✓ ReLU активация с производной")
print(f"  ✓ Softmax активация")
print(f"  ✓ Cross-Entropy loss функция")
print(f"  ✓ Forward pass через все слои (784->128->64->10)")
print(f"  ✓ Backward pass с вычислением градиентов")
print(f"  ✓ Gradient Descent оптимизация")
print(f"  ✓ Mini-batch обучение")
print(f"  ✓ Валидация на отдельном наборе")
print(f"  ✓ Визуализация Loss и Accuracy")
print(f"  ✓ Оценка на тестовом наборе")
print(f"  ✓ Демонстрация предсказаний")
print(f"\n✅ Модель полностью готова к использованию в Google Colab!")
